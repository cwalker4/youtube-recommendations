---
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: no
params:
  analysis_dir: 'analysis'
  depth: 15
  n_splits: 3
  const_depth: 4
  date: '2019-08-12'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      cache = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      cache.lazy = FALSE,
                      # force output to LaTeX (which forces the
                      # imposition of fig.pos) and proper figure alignment
                      fig.align = 'center',
                      fig.pos = 'H')

library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(forcats)
library(readr)
library(purrr)

library(kableExtra)

library(gridExtra)

library(ggplot2)
extrafont::loadfonts(quiet = TRUE)
library(GGally)
library(hrbrthemes)
theme_set(theme_ipsum())
library(atheylab.utils)

knitr::knit_hooks$set(inline = function(x) {
  atheylab.utils::pretty_print_inline(x, decimals = 2)
})

library(texreg)

source(here('scripts/analysis/utils.R'))
```

---
title: "`r params$date` Crawl"
---

In this document we run some basic sanity checks on whether our crawler is behaving as expected. The parameters of this crawl were:

```{r}
tibble(parameter = c("Analysis Directory", "Max Depth", "Split Factor", "Sampling Depth"),
       values = unlist(params)[1:4])

```

```{r}
################################################################################
# data import + tidying
################################################################################

indir <- 'data/derived_data'

# raw video info
read_csv(here::here(indir, params$analysis_dir, 'video_info.csv'), na = c("", -1)) %>%
  mutate_at(vars(category, dislikes, likes, views, n_comments), as.integer) -> video_info

# category id <-> category name crosswalk
category_crosswalk <- read_csv(here::here(indir, 'category_crosswalk.csv'))

# search info dataframe
read_csv(here::here(indir, params$analysis_dir, 'search_info.csv'), na = c("", -1)) %>%
  mutate(search = stringr::str_replace_all(search, ' ', '_'),
         recommendations = sapply(recommendations, format_python_lists, USE.NAMES = FALSE),
         n_recs = map_dbl(recommendations, ~ifelse(anyNA(.x), 0, length(.x)))) -> search_info

if (!is.null(params$date)) {
  search_info <- filter(search_info, scrape_date == params$date)
}
  
```

Let's take a look at the number of videos at each depth, averaged across all our trees. The first possibly concerning thing to note is that we are visiting far fewer videos than expected, based on our search criteria.

```{r vids_per_depth}
expected_nvids <- function(depth, n_splits, const_depth) {
  ex <- rep(n_splits^const_depth, depth+1)
  for (i in 1:const_depth) {
    ex[i] <- n_splits^(i-1)
  }
  return(ex)
}

expected_n <- expected_nvids(params$depth, params$n_splits, params$const_depth)

search_info %>%
  group_by(search, depth) %>%
  tally() %>%
  ungroup() %>%
  group_by(depth) %>%
  summarise(n = mean(n)) -> n_vids_by_depth

n_vids_by_depth %>%
  mutate(expected = expected_n) %>%
  rename(actual = n) %>%
  gather(stat, n, -depth) %>%
  ggplot() +
  geom_col(aes(depth, n, fill = stat), position = 'dodge') 

```

This is not necessarily because our recommendation scraper isn't working - we don't follow a recommendation if we've seen it before. Taking a look at number of recommendations we're getting, it appears that this is the case. 

```{r}
search_info %>%
  filter(depth < params$const_depth) %>%
  count(n_recs) %>%
  mutate(freq = n / sum(n),
         n_recs = factor(n_recs, levels = c(0:3))) %>%
  ggplot(aes(n_recs, freq)) + 
  geom_col() +
  scale_x_discrete(breaks=c(0:3), drop = FALSE)

```

How about after sampling? Looks like things are lining up as expected. 

```{r}
ex_freq <- dbinom(c(0:params$n_splits), size=params$n_splits, prob=1/params$n_splits)
search_info %>%
  mutate(n_recs = factor(n_recs, levels = c(0:3))) %>%
  filter(depth >= params$const_depth) %>%
  count(n_recs, .drop = FALSE) %>%
  mutate(freq = n / sum(n),
         ex_freq = ex_freq) %>%
  select(-n) %>%
  gather(stat, val, -n_recs) %>%
  ggplot(aes(n_recs, val, fill = stat)) + 
  geom_col(position = 'dodge') +
  ylab("Frequency") +
  xlab("Number of recommendations")

```

A possibly more informative plot: average number of recommendations by depth. Recall that, since we are sampling after depth `r params$const_depth`, we want to be following one recommendation on average after that. Things look pretty good.

```{r}
search_info %>%
  mutate(sampling = depth >= params$const_depth) %>%
  group_by(depth, sampling) %>%
  summarise_at("n_recs", mean) %>%
  ggplot(aes(depth, n_recs, fill = sampling)) +
  geom_col()
  

```

How many of our trees are even making it to the max depth?

```{r}
search_info %>%
  distinct(root_video, depth) %>%
  count(depth) %>%
  mutate(p = n / n[which.min(depth)]) %>%
  ggplot(aes(depth, p)) +
  geom_line() +
  scale_x_continuous(breaks=c(0:15)) + 
  ylim(c(0,1)) + 
  ylab('Empirical survival probability')

```



