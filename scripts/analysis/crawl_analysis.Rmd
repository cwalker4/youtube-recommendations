---
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: no
params:
  analysis_dir: 'analysis'
  depth: 20
  n_splits: 4
  const_depth: 5
  date: '2019-08-31'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      cache = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      cache.lazy = FALSE,
                      # force output to LaTeX (which forces the
                      # imposition of fig.pos) and proper figure alignment
                      fig.align = 'center',
                      fig.pos = 'H')

library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(forcats)
library(readr)
library(purrr)

library(kableExtra)

library(gridExtra)

library(ggplot2)
extrafont::loadfonts(quiet = TRUE)
library(GGally)
library(hrbrthemes)
theme_set(theme_ipsum())
library(atheylab.utils)

knitr::knit_hooks$set(inline = function(x) {
  atheylab.utils::pretty_print_inline(x, decimals = 2)
})

library(texreg)

source(here('scripts/analysis/utils.R'))

con <- RSQLite::dbConnect(SQLite(), here('data/crawl.sqlite'))
```

---
title: "`r params$date` Crawl"
---

In this document we run some basic sanity checks on whether our crawler is behaving as expected. The parameters of this crawl were:

```{r}
tibble(parameter = c("Analysis Directory", "Max Depth", "Split Factor", "Sampling Depth"),
       values = unlist(params)[1:4])

```

```{r}
################################################################################
# data import + tidying
################################################################################

# raw video info
video_info <- dbGetQuery(con, "SELECT * FROM videos")
video_info <- mutate_at(video_info, vars(category, dislikes, likes, views, n_comments), as.integer)

# recommendations data
sql <- "
select video_id, r.search_id, recommendation, r.depth, s.date,
       COUNT(recommendation) OVER (
         PARTITION BY video_id, r.search_id
         ) n_recs
FROM recommendations r
LEFT JOIN searches s
  ON r.search_id=s.search_id
"
recs <- dbGetQuery(con, sql)

if (!is.null(params$date)) {
  recs <- filter(recs, date == params$date)
}
  
```

Let's take a look at the number of videos at each depth, averaged across all our trees. The first possibly concerning thing to note is that we are visiting far fewer videos than expected, based on our search criteria.

```{r vids_per_depth}
expected_nvids <- function(depth, n_splits, const_depth) {
  ex <- rep(n_splits^const_depth, depth)
  for (i in 1:const_depth) {
    ex[i] <- n_splits^(i)
  }
  return(ex)
}

expected_n <- expected_nvids(params$depth, params$n_splits, params$const_depth)

recs %>%
  filter(!is.na(recommendation)) %>%
  mutate(depth = depth + 1) %>%
  group_by(search_id, depth) %>%
  tally() %>%
  ungroup() %>%
  group_by(depth) %>%
  summarise(n = mean(n)) -> n_vids_by_depth

n_vids_by_depth %>%
  mutate(expected = expected_n) %>%
  rename(actual = n) %>%
  gather(stat, n, -depth) %>%
  ggplot() +
  geom_col(aes(depth, n, fill = stat), position = 'dodge') 

```

This is not necessarily because our recommendation scraper isn't working - we don't follow a recommendation if we've seen it before. Taking a look at number of recommendations we're getting, it appears that this is the case. 

```{r}
recs %>%
  filter(depth < params$const_depth) %>%
  distinct(video_id, n_recs) %>%
  count(n_recs, .drop = FALSE) %>%
  mutate(freq = n / sum(n),
         n_recs = factor(n_recs, levels = c(0:3))) %>%
  ggplot(aes(n_recs, freq)) + 
  geom_col() +
  scale_x_discrete(breaks=c(0:4), drop = FALSE)

```

How about after sampling? Looks like things are lining up as expected. 

```{r}
ex_freq <- dbinom(c(0:params$n_splits), size=params$n_splits, prob=1/params$n_splits)
recs %>%
  filter(depth >= params$const_depth) %>%
  distinct(video_id, n_recs) %>%
  count(n_recs, .drop = FALSE) %>%
  mutate(freq = n / sum(n),
         ex_freq = ex_freq) -> freqs

freqs %>%
  select(-n) %>%
  gather(stat, val, -n_recs) %>%
  ggplot(aes(n_recs, val, fill = stat)) + 
  geom_col(position = 'dodge') +
  ylab("Frequency") +
  xlab("Number of recommendations")

tstat <- sum((freqs$freq - freqs$ex_freq)^2 / freqs$ex_freq)
pval <- pchisq(tstat, df = nrow(freqs) - 1, lower.tail = FALSE)

```

Formalizing this with a chi-squared test comparing the expected and empirical distributions we get $\chi^2_4=$ `r tstat` and associated p-value of `r pval`.  A possibly more informative plot: average number of recommendations by depth. Recall that, since we are sampling after depth `r params$const_depth`, we want to be following one recommendation on average after that. Things look pretty good.

```{r}
recs %>%
  distinct(video_id, depth, n_recs) %>%
  mutate(sampling = depth >= params$const_depth) %>%
  group_by(depth, sampling) %>%
  summarise_at("n_recs", mean) %>%
  ggplot(aes(depth, n_recs, fill = sampling)) +
  geom_col()
  

```

How many of our trees are even making it to the max depth?

```{r}
recs %>%
  distinct(search_id, depth) %>%
  count(depth) %>%
  mutate(p = n / n[which.min(depth)]) %>%
  ggplot(aes(depth, p)) +
  geom_line() +
  ylim(c(0,1)) + 
  ylab('Empirical survival probability')

```



