---
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: no
params:
  analysis_dir: 'analysis'
  depth: 15
  n_splits: 3
  const_depth: 4
  date: '2019-08-12'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      cache.lazy = FALSE,
                      # force output to LaTeX (which forces the
                      # imposition of fig.pos) and proper figure alignment
                      fig.align = 'center',
                      fig.pos = 'H')

library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(forcats)
library(readr)

library(kableExtra)

library(gridExtra)

library(ggplot2)
extrafont::loadfonts(quiet = TRUE)
library(GGally)
library(hrbrthemes)
theme_set(theme_ipsum())
library(atheylab.utils)

knitr::knit_hooks$set(inline = function(x) {
  atheylab.utils::pretty_print_inline(x, decimals = 2)
})

library(texreg)

source(here('scripts/analysis/utils.R'))
```

---
title: "Sampling Exploration for `r params$date` Crawl"
---

In the new scraper we start uniformly sampling from the recommended videos so we are following one recommendation in expectation. Specifically, if at each node we follow $n$ recommendations, past our critical depth we follow each of the $n$ recommendations with probability $\frac{1}{n}$. The goal is to approach something resembling a stationary distribution (w.r.t. features of interest) without the exponential growth in number of videos at each depth. 


```{r}
################################################################################
# data import + tidying
################################################################################

indir <- 'data/derived_data'

# raw video info
read_csv(here::here(indir, params$analysis_dir, 'video_info.csv'), na = c("", -1)) %>%
  mutate_at(vars(category, dislikes, likes, views, n_comments), as.integer) -> video_info

# category id <-> category name crosswalk
category_crosswalk <- read_csv(here::here(indir, 'category_crosswalk.csv'))

# search info dataframe
read_csv(here::here(indir, params$analysis_dir, 'search_info.csv'), na = c("", -1)) %>%
  mutate(search = stringr::str_replace_all(search, ' ', '_'),
         recommendations = sapply(recommendations, format_python_lists),
         n_recs = sapply(recommendations, length)) -> search_info

if (!is.null(params$date)) {
  search_info <- filter(search_info, scrape_date == params$date)
}
  
```

Let's take a look at the number of videos at each depth, averaged across all our trees. The first concerning thing to note is that we are visiting far fewer videos than expected, based on our search criteria.

```{r vids_per_depth}
expected_nvids <- function(depth, n_splits, const_depth) {
  ex <- rep(n_splits^const_depth, depth+1)
  for (i in 1:const_depth) {
    ex[i] <- n_splits^(i-1)
  }
  return(ex)
}

expected_n <- expected_nvids(params$depth, params$n_splits, params$const_depth)

search_info %>%
  group_by(search, depth) %>%
  tally() %>%
  ungroup() %>%
  group_by(depth) %>%
  summarise(n = mean(n)) -> n_vids_by_depth

n_vids_by_depth %>%
  mutate(expected = expected_n) %>%
  rename(actual = n) %>%
  gather(stat, n, -depth) %>%
  ggplot() +
  geom_col(aes(depth, n, fill = stat), position = 'dodge') 

```

This is not necessarily because our recommendation scraper isn't working - we don't follow a recommendation if we've seen it before. Taking a look at number of recommendations we're getting, it appears that this is the case. 

```{r}
search_info %>%
  filter(depth < params$const_depth) %>%
  count(n_recs) %>%
  mutate(freq = n / sum(n),
         n_recs = factor(n_recs, levels = c(0:3))) %>%
  ggplot(aes(n_recs, freq)) + 
  geom_col() +
  scale_x_discrete(breaks=c(0:3), drop = FALSE)

```

How about after sampling? Looks like things are lining up as expected. 

```{r}
ex_freq <- dbinom(c(0:params$n_splits), size=params$n_splits, prob=1/params$n_splits)
search_info %>%
  filter(depth >= params$const_depth) %>%
  count(n_recs) %>%
  mutate(freq = n / sum(n),
         ex_freq = ex_freq) %>%
  select(-n) %>%
  gather(stat, val, -n_recs) %>%
  ggplot(aes(n_recs, val, fill = stat)) + 
  geom_col(position = 'dodge') +
  ylab("Frequency") +
  xlab("Number of recommendations")

```

How many of our trees are even making it to the max depth?

```{r}
search_info %>%
  distinct(root_video, depth) %>%
  count(depth) %>%
  mutate(p = n / n[which.min(depth)]) %>%
  ggplot(aes(depth, p)) +
  geom_line() +
  scale_x_continuous(breaks=c(0:15)) + 
  ylim(c(0,1)) + 
  ylab('Empirical survival probability')

```



