---
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: no
params:
  analysis_dir: 'analysis'
  depth: 15
  n_splits: 3
  const_depth: 4
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      cache.lazy = FALSE,
                      # force output to LaTeX (which forces the
                      # imposition of fig.pos) and proper figure alignment
                      fig.align = 'center',
                      fig.pos = 'H')

library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(forcats)
library(readr)
library(purrr)
library(RSQLite)

library(kableExtra)

library(gridExtra)

library(ggplot2)
extrafont::loadfonts(quiet = TRUE)
library(GGally)
library(hrbrthemes)
theme_set(theme_ipsum())
library(atheylab.utils)

knitr::knit_hooks$set(inline = function(x) {
  atheylab.utils::pretty_print_inline(x, decimals = 2)
})

library(texreg)

source(here('scripts/analysis/utils.R'))
```

---
title: "Mixing Analysis"
---

In this document we take a stab at looking at analyzing our data from a mixing perspective: the idea is that there is some stationary distribution on the recommendation graph, and it might be interesting to know how quickly we approach that stationary distribution. Even more interesting is if there is a difference in how quickly/slowly we approach the stationary distribution, depending on where we start. This could give us some notion of whether recommendations systematically point towards/away from certain types of content. 

```{r}
################################################################################
# data import + tidying
################################################################################

# set up database connection
con = dbConnect(SQLite(), here('data/crawl.sqlite'))

# video info with category name and degrees joined in
sql <- "
WITH 
video_degrees AS (
  SELECT recommendation AS video_id, COUNT(video_id) AS in_degree
  FROM recommendations
  GROUP BY recommendation
),
video_depths AS (
  SELECT DISTINCT video_id, search_id, depth
  FROM recommendations
)
SELECT 
v.video_id, v.search_id, channel_id, title, postdate, views, likes, dislikes, n_comments, in_degree,
c.category_name AS category, depth
  FROM videos v
LEFT JOIN video_degrees d
  ON v.video_id=d.video_id
LEFT JOIN video_categories c
  ON v.category=c.category_id
LEFT JOIN video_depths vd
  ON v.video_id=vd.video_id
  AND v.search_id=vd.search_id"
video_info <- dbGetQuery(con, sql)
video_info$p_like <- video_info$likes / (video_info$likes + video_info$dislikes)
video_info$postdate <- as.Date(video_info$postdate)

numeric_cols <- c('views', 'likes', 'dislikes', 'n_comments', 'in_degree', 'p_like')
video_info <- mutate_at(video_info, vars(numeric_cols), as.numeric)

# recommendations dataframe 
rec_info <- dbReadTable(con, 'recommendations')

# import the channel-leaning information
channel_leanings <- dbGetQuery(con, "SELECT * FROM channel_leanings")

# merge together and add column for root leaning
video_info %>%
  left_join(channel_leanings, by = 'channel_id') %>%
  select(video_id, search_id, depth, channel_id, leaning) %>%
  group_by(search_id) %>%
  mutate(root_leaning = leaning[which.min(depth)],
         leaning = factor(leaning, levels = c("L", "C", "R"), ordered = TRUE)) %>%
  ungroup() -> search_leanings

```

As a first pass, we'll make the (strong) assumption that the last five levels of our search trees characterize the stationary distribution for variables of interest - let's call this $Q(x)$, where $x$ is the variable. Then we can measure convergence to this distribution using KL divergence:

$$
D_{KL}(P_i || Q) = \sum_{x \in X}P_i(x) \log\bigg(\frac{P_i(x)}{Q(x)}\bigg)
$$

where $P_i(x)$ is the distribution of variable $x$ at depth $i$ of the tree. Let's do this first with political leaning. We have the following proportions of L/C/R leaning videos by depth:

```{r}
search_leanings %>%
  filter(!is.na(leaning)) %>%
  group_by(depth, leaning) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ggplot(aes(depth, freq, fill = leaning)) +
  geom_col() +
  scale_fill_manual(values = c("blue", "purple", "red"))

```

There's not a ton of movement, and this figure doesn't really support the assumption that we reach the stationary distribution after depth 10, but let's continue for now. Let's take the last 10 levels and use those to construct $Q(x)$.

```{r}
search_leanings %>%
  filter(!is.na(leaning),
         depth >= 10) %>%
  count(leaning) %>%
  mutate(freq = n / sum(n)) -> q_x
q_x
```

Cool. Now let's calculate the KL divergence between this distribution and the distribution of leanings for each depth of our crawls.

```{r}
# F'n to get the divergence between categorical dists p and q. No safety checks.
kl_divergence <- function(p, q) {
  sum(p * log(p/q))
}

# get the distributions for each depth
search_leanings %>%
  filter(!is.na(leaning)) %>%
  count(depth, leaning) %>%
  group_by(depth) %>%
  mutate(freq = n / sum(n)) %>%
  summarise(p_x = list(freq)) -> p_by_depth

# calculate the divergence between each depth and q_x in the chunk above
p_by_depth <- mutate(p_by_depth, divergence = map_dbl(p_x, ~kl_divergence(.x, q_x$freq))) 
  
# display
select(p_by_depth, depth, divergence)

```


```{r}
p_by_depth %>%
  ggplot(aes(depth, divergence)) +
  geom_line()

```

What happens if we break this up by leaning of the root node? Recall this figure, where it appears that the left-leaning videos converge faster to the stationary distribution than right-leaning videos.

```{r}
search_leanings %>%
  filter(!is.na(leaning), !is.na(root_leaning)) %>%
  count(root_leaning, depth, leaning) %>%
  group_by(root_leaning, depth) %>%
  mutate(freq = n / sum(n)) %>%
  ggplot(aes(depth, freq, fill = leaning)) +
  geom_col() + 
  scale_fill_manual(values = c("blue", "purple", "red")) + 
  facet_wrap(~root_leaning)

```

Let's take a closer look at what's going on with the KL divergence.

```{r}
# get the distributions for each depth
search_leanings %>%
  filter(!is.na(leaning), !is.na(root_leaning)) %>%
  count(root_leaning, depth, leaning) %>%
  group_by(root_leaning, depth) %>%
  mutate(freq = n / sum(n)) %>%
  summarise(p_x = list(freq)) -> p_by_depth_root

# calculate the divergence between each depth and q_x in the chunk above
p_by_depth_root <- mutate(p_by_depth_root, divergence = map_dbl(p_x, ~kl_divergence(.x, q_x$freq))) 

# plot it
p_by_depth_root %>%
  ggplot(aes(depth, divergence, colour = root_leaning)) +
  geom_line() + 
  scale_color_manual(values = c("blue", "purple", "red"))

```

# Questioning Assumptions

There are a few weird things going on above: what's with the spike in KL divergence? Why does the distribution of L/C/R videos across our searches seem to stabilize around depth 5, but then get wonky after depth 15? One possible explanation is that our analysis of our data as a tree is not consistent with our data. In particular, we are dealing with a "truncated" tree as a result of the crawler not getting the recommendations for a video if we have seen it before. Consequently, as we go down our tree we are following fewer and fewer edges (increasing probability of bumping into something we've seen before), so our sample becomes more and more sparse at greater depths. This is reflected in the number of videos we have, on average, at each depth. The following figure is copied from `crawl_analysis.Rmd`: by the time we reach depth 18-20, we are on average visiting the same number of videos as in depth 1-3! 

Possible solution: recreate the full BFS tree before running this analysis. Analyze a dataframe where there are as many videos at each depth as there would have been if we had followed every recommendation. [See Issue #1 in the GitHub](https://github.com/cwalker4/youtube-recommendations/issues/1).

```{r}
# recommendations data
sql <- "
select video_id, r.search_id, recommendation, r.depth, s.date,
       count(recommendation) OVER (
         PARTITION BY video_id, r.search_id
         ) n_recs
FROM recommendations r
LEFT JOIN searches s
  ON r.search_id=s.search_id
"
recs <- dbGetQuery(con, sql)

recs %>%
  mutate(depth = depth + 1) %>%
  group_by(search_id, depth) %>%
  tally() %>%
  ungroup() %>%
  group_by(depth) %>%
  summarise(n = mean(n)) -> n_vids_by_depth

n_vids_by_depth %>%
  ggplot(aes(depth, n, label = round(n, 1))) +
  geom_col() +
  geom_label(label.size=0.1)
```





