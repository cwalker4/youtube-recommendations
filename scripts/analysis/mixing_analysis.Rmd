---
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: no
params:
  analysis_dir: 'analysis'
  depth: 15
  n_splits: 3
  const_depth: 4
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      cache.lazy = FALSE,
                      # force output to LaTeX (which forces the
                      # imposition of fig.pos) and proper figure alignment
                      fig.align = 'center',
                      fig.pos = 'H')

library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(forcats)
library(readr)
library(purrr)

library(kableExtra)

library(gridExtra)

library(ggplot2)
extrafont::loadfonts(quiet = TRUE)
library(GGally)
library(hrbrthemes)
theme_set(theme_ipsum())
library(atheylab.utils)

knitr::knit_hooks$set(inline = function(x) {
  atheylab.utils::pretty_print_inline(x, decimals = 2)
})

library(texreg)

source(here('scripts/analysis/utils.R'))
```

---
title: "Mixing Analysis"
---

In this document we take a stab at looking at analyzing our data from a mixing perspective: the idea is that there is some stationary distribution on the recommendation graph, and it might be interesting to know how quickly we approach that stationary distribution. Even more interesting is if there is a difference in how quickly/slowly we approach the stationary distribution, depending on where we start. This could give us some notion of whether recommendations systematically point towards/away from certain types of content. 

```{r}
################################################################################
# data import + tidying
################################################################################

# set up database connection
con = dbConnect(SQLite(), here('data/crawl.sqlite'))

# video info with category name and degrees joined in
sql <- "
WITH 
video_degrees AS (
  SELECT recommendation AS video_id, COUNT(video_id) AS in_degree
  FROM recommendations
  GROUP BY recommendation
),
video_depths AS (
  SELECT DISTINCT video_id, search_id, depth
  FROM recommendations
)
SELECT 
v.video_id, v.search_id, channel_id, title, postdate, views, likes, dislikes, n_comments, in_degree,
c.category_name AS category, depth
  FROM videos v
LEFT JOIN video_degrees d
  ON v.video_id=d.video_id
LEFT JOIN video_categories c
  ON v.category=c.category_id
LEFT JOIN video_depths vd
  ON v.video_id=vd.video_id
  AND v.search_id=vd.search_id"
video_info <- dbGetQuery(con, sql)
video_info$p_like <- video_info$likes / (video_info$likes + video_info$dislikes)
video_info$postdate <- as.Date(video_info$postdate)

numeric_cols <- c('views', 'likes', 'dislikes', 'n_comments', 'in_degree', 'p_like')
video_info <- mutate_at(video_info, vars(numeric_cols), as.numeric)

# recommendations dataframe 
rec_info <- dbReadTable(con, 'recommendations')

# import the channel-leaning information
channel_leanings <- dbGetQuery(con, "SELECT * FROM channel_leanings")

# merge together and add column for root leaning
video_info %>%
  left_join(channel_leanings, by = 'channel_id') %>%
  select(video_id, search_id, depth, channel_id, leaning) %>%
  group_by(search_id) %>%
  mutate(root_leaning = leaning[which.min(depth)],
         leaning = factor(leaning, levels = c("L", "C", "R"), ordered = TRUE)) %>%
  ungroup() -> search_leanings

```

As a first pass, we'll make the (strong) assumption that the last five levels of our search trees characterize the stationary distribution for variables of interest - let's call this $Q(x)$, where $x$ is the variable. Then we can measure convergence to this distribution using KL divergence:

$$
D_{KL}(P_i || Q) = \sum_{x \in X}P_i(x) \log\bigg(\frac{P_i(x)}{Q(x)}\bigg)
$$

where $P_i(x)$ is the distribution of variable $x$ at depth $i$ of the tree. Let's do this first with political leaning. We have the following proportions of L/C/R leaning videos by depth:

```{r}
search_leanings %>%
  filter(!is.na(leaning)) %>%
  group_by(depth, leaning) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ggplot(aes(depth, freq, fill = leaning)) +
  geom_col() +
  scale_fill_manual(values = c("blue", "purple", "red"))

```

There's not a ton of movement, and this figure doesn't really support the assumption that we reach the stationary distribution after depth 10, but let's continue for now. Let's take the last 10 levels and use those to construct $Q(x)$.

```{r}
search_leanings %>%
  filter(!is.na(leaning),
         depth >= 10) %>%
  count(leaning) %>%
  mutate(freq = n / sum(n)) -> q_x
q_x
```

Cool. Now let's calculate the KL divergence between this distribution and the distribution of leanings for each depth of our crawls.

```{r}
# F'n to get the divergence between categorical dists p and q. No safety checks.
kl_divergence <- function(p, q) {
  sum(p * log(p/q))
}

# get the distributions for each depth
search_leanings %>%
  filter(!is.na(leaning)) %>%
  count(depth, leaning) %>%
  group_by(depth) %>%
  mutate(freq = n / sum(n)) %>%
  summarise(p_x = list(freq)) -> p_by_depth

# calculate the divergence between each depth and q_x in the chunk above
p_by_depth <- mutate(p_by_depth, divergence = map_dbl(p_x, ~kl_divergence(.x, q_x$freq))) 
  
# display
select(p_by_depth, depth, divergence)

```


```{r}
p_by_depth %>%
  ggplot(aes(depth, divergence)) +
  geom_line()

```

What happens if we break this up by leaning of the root node? Recall this figure, where it appears that the left-leaning videos converge faster to the stationary distribution than right-leaning videos.

```{r}
search_leanings %>%
  filter(!is.na(leaning), !is.na(root_leaning)) %>%
  count(root_leaning, depth, leaning) %>%
  group_by(root_leaning, depth) %>%
  mutate(freq = n / sum(n)) %>%
  ggplot(aes(depth, freq, fill = leaning)) +
  geom_col() + 
  scale_fill_manual(values = c("blue", "purple", "red")) + 
  facet_wrap(~root_leaning)

```

Let's take a closer look at what's going on with the KL divergence.

```{r}
# get the distributions for each depth
search_leanings %>%
  filter(!is.na(leaning), !is.na(root_leaning)) %>%
  count(root_leaning, depth, leaning) %>%
  group_by(root_leaning, depth) %>%
  mutate(freq = n / sum(n)) %>%
  summarise(p_x = list(freq)) -> p_by_depth_root

# calculate the divergence between each depth and q_x in the chunk above
p_by_depth_root <- mutate(p_by_depth_root, divergence = map_dbl(p_x, ~kl_divergence(.x, q_x$freq))) 

# plot it
p_by_depth_root %>%
  ggplot(aes(depth, divergence, colour = root_leaning)) +
  geom_line() + 
  scale_color_manual(values = c("blue", "purple", "red"))

```






