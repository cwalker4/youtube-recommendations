---
output:
  html_document:
    df_print: paged
  pdf_document:
    number_sections: no
params:
  date: 'NULL'
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      cache = FALSE, 
                      warning = FALSE,
                      message = FALSE,
                      cache.lazy = FALSE,
                      # force output to LaTeX (which forces the
                      # imposition of fig.pos) and proper figure alignment
                      fig.align = 'center',
                      fig.pos = 'H')

library(dplyr)
library(here)
library(tidyr)
library(stringr)
library(forcats)
library(broom)
library(readr)
library(jsonlite)
library(RSQLite)

library(kableExtra)

library(gridExtra)

library(ggplot2)
extrafont::loadfonts(quiet = TRUE)
library(GGally)
library(hrbrthemes)
theme_set(theme_ipsum())
library(atheylab.utils)

knitr::knit_hooks$set(inline = function(x) {
  atheylab.utils::pretty_print_inline(x, decimals = 2)
})

library(texreg)

```

---
title: "Project Analysis"
---

This notebook runs the analysis associated with the course project for MS&E 234. Very basic preliminary poking at the data collected by the scraper. 

```{r}
################################################################################
# data import + tidying
################################################################################

indir <- 'data/derived_data'

# set up database connection
con = dbConnect(SQLite(), here('data/crawl.sqlite'))

# video info with category name and degrees joined in
sql <- "
WITH 
video_degrees AS (
  SELECT recommendation AS video_id, COUNT(video_id) AS in_degree
  FROM recommendations
  GROUP BY recommendation
),
video_depths AS (
  SELECT DISTINCT video_id, search_id, depth
  FROM recommendations
)
SELECT 
v.video_id, v.search_id, channel_id, title, postdate, views, likes, dislikes, n_comments, in_degree,
c.category_name AS category, depth
  FROM videos v
LEFT JOIN video_degrees d
  ON v.video_id=d.video_id
LEFT JOIN video_categories c
  ON v.category=c.category_id
LEFT JOIN video_depths vd
  ON v.video_id=vd.video_id
  AND v.search_id=vd.search_id"
video_info <- dbGetQuery(con, sql)
video_info$p_like <- video_info$likes / (video_info$likes + video_info$dislikes)
video_info$postdate <- as.Date(video_info$postdate)

numeric_cols <- c('views', 'likes', 'dislikes', 'n_comments', 'in_degree', 'p_like')
video_info <- mutate_at(video_info, vars(numeric_cols), as.numeric)

# recommendations dataframe 
rec_info <- dbReadTable(con, 'recommendations')
  
```

Basic summaries across all our variables. In a recent change to the crawler we store information on each video-tree pairing, instead of each video: the idea is to make sure we are analyzing video information as it was when the crawler visited it. Relevant to the below is that this means our means are somehwat weighted by the number of times we visit a particular video. 

```{r}
rec_info %>%
  distinct(video_id, search_id, depth) %>%
  left_join(video_info, by = c('video_id', 'search_id')) -> video_info_w_depth

video_info %>%
  select(views, likes, dislikes, n.comments = n_comments) %>%
  summarise_all(list(min = ~min(., na.rm = TRUE),
                     median = ~median(., na.rm = TRUE),
                     max = ~max(., na.rm = TRUE),
                     mean = ~mean(., na.rm = TRUE),
                     sd = ~sd(., na.rm = TRUE))) -> summary_stats

summary_stats %>%
  mutate_all(as.numeric) %>%
  gather(stat, val) %>%
  tidyr::separate(stat, into = c("var", "stat"), sep = "_", extra = "merge") %>%
  spread(stat, val) %>%
  select(Variable = var, mean, median, min, max, sd) %>%
  kable(format = "latex",
        caption = "Summaries",
        booktabs = TRUE,
        digits = 2) %>%
  kable_styling()

```


Now asking the question: did any features of our videos change (on average) as we traversed the trees?

```{r}
# get average statistics by search depth
video_info %>%
  group_by(depth) %>%
  summarise_at(vars(likes, n_comments, views, p_like),
               function(x) mean(x, na.rm = TRUE)) %>%
  gather(stat, mean, -depth) -> mean_by_depth

video_info %>%
  group_by(depth) %>%
  summarise_at(vars(likes, n_comments, views, p_like),
               function(x) sd(x, na.rm = TRUE)) %>%
  gather(stat, sd, -depth) -> sd_by_depth

mean_by_depth %>%
  left_join(sd_by_depth, by = c('depth', 'stat')) -> stats_by_depth

stats_by_depth %>%
  ggplot(aes(depth, mean)) +
  geom_line() +
  facet_wrap(~ stat, scales="free_y")

```

This gives us an overview. For the sake of less ugly plots let's make these individually

```{r}
stats_by_depth %>%
  filter(stat == "views") %>%
  ggplot(aes(depth, mean)) +
  geom_line() +
  geom_ribbon(aes(ymin=mean-sd, ymax=mean+sd), alpha=0.1) +
  ylab("Average Views") +
  xlab("BFS Depth") +
  scale_x_continuous(breaks=1:max(stats_by_depth$depth))

```


```{r}
stats_by_depth %>%
  filter(stat == "p_like") %>%
  mutate(mean = 1 - mean,
         stat = "p_dislike") %>%
  select(-sd) -> dislikes

stats_by_depth %>%
  bind_rows(dislikes) %>%
  filter(stat %in% c("p_like", "p_dislike")) %>%
  mutate(stat = ifelse(stat == "p_like", "Likes", "Dislikes")) %>%
  ggplot(aes(depth, mean, fill=stat)) +
  geom_area() +
  ylab("Fraction of Likes/Dislikes") +
  xlab("BFS Depth") + 
  scale_fill_manual(name = "", values = c("red", "darkgreen")) 

```

What happens to our categories as we go deeper?

```{r}
video_info %>%
  mutate_at("category", as.factor) %>%
  group_by(depth, category) %>%
  summarise(n = n()) %>%
  mutate(category_freq = n / sum(n)) -> category_freq

category_freq %>%
  ggplot(aes(depth, category_freq, fill = category)) + 
  geom_col() +
  xlab("BFS Depth") + 
  ylab("Frequency")

```



```{r}
# post date by depth
video_info %>%
  group_by(search_id) %>%
  mutate(root_date = postdate[which(depth == 0)]) %>%
  mutate(time_diff = difftime(postdate, root_date, units="days")) %>%
  ungroup() %>%
  select(depth, search_id, time_diff) -> time_diffs

time_diffs %>%
  group_by(depth) %>%
  summarize(mean = mean(time_diff, na.rm = TRUE),
           se = sd(time_diff, na.rm = TRUE) / sqrt(length(time_diff))) %>%
  ggplot(aes(depth, mean)) +
  geom_line() +
  geom_ribbon(aes(ymin=mean-se, ymax=mean+se), alpha=0.1) +
  ylab("Time Difference (days)")

```

```{r}
video_info %>%
  mutate(video_age = as.numeric(difftime(as.Date(Sys.Date(), format="%Y-%m-%d"), postdate, units="days"))) %>%
  mutate(age_bin = cut(video_age, seq(0, 4500, 50), labels=FALSE)) %>%
  mutate(age_bin = age_bin * 50) %>%
  group_by(age_bin) %>%
  summarise(mean = mean(views, na.rm = TRUE),
            se = sd(views, na.rm = TRUE) / sqrt(length(views))) %>%
  ungroup() %>%
  ggplot(aes(age_bin, mean)) +
  geom_point() +
  ylab("Views") + 
  xlab("Video Age (days)") +
  xlim(c(0, 2500))


```

# Graphy things

```{r}
# degree distribution
#n_videos <- nrow(degrees)
n_videos <- length(unique(video_info$video_id))

video_info %>%
  distinct(video_id, in_degree) %>%
  group_by(in_degree) %>%
  summarise(p = n() / n_videos) %>%
  ggplot(aes(in_degree, p)) + 
  geom_point() + 
  scale_y_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),
                labels = scales::trans_format("log10", scales::math_format(10^.x))) + 
  scale_x_log10(breaks = scales::trans_breaks("log10", function(x) 10^x),
                labels = scales::trans_format("log10", scales::math_format(10^.x))) +
  ylab("P(in-degree)") + 
  xlab("in-degree") 

```

Look at characteristics by in-degree. First: what are the videos with the highest in-degree?

```{r}
video_info %>%
  distinct(video_id, in_degree) %>%
  top_n(10, in_degree) %>%
  arrange(-in_degree) %>%
  left_join(video_info %>% select(video_id, title), by = "video_id") %>%
  kable(format = "latex",
        caption = "Top Videos by In-Degree",
        col.names = c("Video", "In-Degree", "Title"),
        booktabs = TRUE,
        digits = 2) %>%
  kable_styling(latex_options = c("scale_down"))
  
```

How do things vary with in-degree?

```{r}
video_info %>%
  select(-category, -channel_id, -postdate, -title) %>%
  gather(stat, val, -c("video_id", "in_degree", "search_id")) %>%
  filter(!is.na(val)) -> deg_stats_long
  
deg_stats_long %>%
  filter(stat %in% c('dislikes', 'likes', 'n_comments', 'views', 'p_like')) %>%
  mutate_at('val', as.numeric) %>%
  group_by(in_degree, stat) %>%
  dplyr::summarize(mean = mean(val, na.rm = TRUE),
            se = sd(val, na.rm = TRUE) / sqrt(length(val)),
            count = n()) -> deg_stats

```

```{r}
deg_stats %>%
  filter(stat %in% c("views", "p_like")) %>%
  mutate(stat = case_when(stat == "views" ~ "Views",
                          stat == "p_like" ~ "Like Fraction")) %>%
  ggplot(aes(in_degree, mean, size = count)) +
  geom_point(alpha = 0.5) + 
  facet_wrap(~stat, nrow=2, scales = "free_y") +
  xlim(c(0, 40)) + 
  ylab("Views") +
  xlab("In Degree")
```

# Polarization

The motivation for this project was to get a sese of the polarization of the content that the Youtube recommendation engine points towards. I've scraped a few websites and compiled a list of media outlets / political sources and their political leaning: left, center, or right. I want to make a figure / do some analysis that captures the probability you land on a left- or right-leaning video conditional on your starting query -- I seeded my scrapes with the names of left- or right-leaning politicians. There are two ways I might condition:

1. P(landing on bias _b_ at level _d_ | started with _b_ leaning query)
2. P(landing on bias _b_ at level _d_ | started by _b_ leaning video)

```{r}
# import the channel-leaning information
channel_leanings <- dbGetQuery(con, "SELECT * FROM channel_leanings")

video_info %>%
  left_join(channel_leanings, by = 'channel_id') %>%
  select(video_id, search_id, depth, channel_id, leaning) -> search_leanings

```

First look at the counts of R vs L vs C leaning videos. Looks like we have mostly left leaning videos.

```{r}
search_leanings %>%
  count(leaning) %>%
  mutate(freq = n / sum(n)) -> leaning_freqs

search_leanings %>%
  distinct(video_id, leaning) %>%
  count(leaning) %>%
  mutate(frac_videos = n / sum(n)) %>%
  rename(n_videos = n) -> video_leaning_counts

search_leanings %>%
  distinct(channel_name, leaning) %>%
  count(leaning) %>%
  mutate(frac_channels = n / sum(n)) %>%
  rename(n_channels = n) -> channel_leaning_counts

channel_leaning_counts %>%
  left_join(video_leaning_counts, by = 'leaning') %>%
  kable(format = "latex",
        caption = "Summaries",
        booktabs = TRUE,
        digits = 2,
        col.names = c("Leaning", "No. Channels", "Frac. Channels", "No. Videos", "Frac. Videos")) %>%
  kable_styling()

```

What are the frequencies for just the base videos?

```{r}
search_leanings %>%
  filter(depth == 0) %>%
  count(leaning) %>%
  mutate(freq = n / sum(n)) -> base_leanings

base_leanings %>%
  kable(format = "latex",
        caption = "Summaries",
        booktabs = TRUE,
        digits = 2,
        col.names = c("Leaning", "Count", "Proportion")) %>%
  kable_styling()
```

Plot political leaning of channels by depth:

```{r}
search_leanings %>%
  filter(!is.na(leaning)) %>%
  group_by(depth, leaning) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) -> leaning_by_depth

leaning_by_depth %>%
  mutate(leaning = case_when(leaning == "C" ~ "Center",
                             leaning == "L" ~ "Left",
                             leaning == "R" ~ "Right")) %>%
  ggplot() + 
  geom_col(aes(depth, freq, fill = leaning)) + 
  scale_fill_manual(values = c("purple", "blue", "red")) +
  ylab("Frequency") + 
  xlab("BFS Depth")

```

```{r}
search_leanings %>%
  group_by(search_id) %>%
  mutate(seed_leaning = leaning[which.min(depth)]) %>%
  ungroup() %>%
  filter(seed_leaning %in% c('L', 'R'),
         !is.na(leaning)) %>%
  group_by(seed_leaning, depth, leaning) %>%
  summarise(n = n()) %>%
  mutate(freq = n / sum(n)) %>%
  ungroup() %>%
  mutate(seed_leaning = ifelse(seed_leaning == 'L', 'Left-Leaning Seed', 'Right-Leaning Seed'),
       leaning = case_when(leaning == "C" ~ "Center",
                           leaning == "L" ~ "Left",
                           leaning == "R" ~ "Right")) %>%
  ggplot() + 
  geom_col(aes(depth, freq, fill = leaning)) + 
  scale_fill_manual(values = c("purple", "blue", "red")) +
  facet_wrap(~ seed_leaning) +
  ylab("Frequency") + 
  xlab("BFS Depth")

```


Isolation index? Count the relative frequency of different leanings (a root video is defined as "R" if it is a right-leaning query) arriving at a  particular video. Then calculate the isolation index a la Getnzkow & Alcott?

# Isolation Index

```{r}
# get the conservative and liberal counts for each channel. Consider each tree as a search
# by a single user. The user's political leaning is defined as the leaning of the very first (root) video
# in the tree.
search_leanings %>%
  group_by(search_id) %>%
  mutate(seed_leaning = leaning[which.min(depth)]) %>%
  ungroup() %>%
  filter(seed_leaning %in% c('L', 'R')) %>%
  mutate(seed_leaning = factor(seed_leaning, levels=c('L', 'R'))) %>%
  count(channel_id, seed_leaning, .drop = FALSE) -> leaning_visit_counts

# get the conservative and liberal shares for each channel
leaning_visit_counts %>%
  tidyr::spread(seed_leaning, n) %>%
  rename(n_l = L, n_r = R) %>%
  mutate(p_l = (n_l) / (n_l + n_r),
         p_r = (n_r) / (n_l + n_r)) -> leaning_info

# get the total number of liberal/conservative seed videos.
search_leanings %>%
  group_by(search_id) %>%
  mutate(seed_leaning = leaning[which.min(depth)]) %>%
  ungroup() %>%
  count(seed_leaning) -> total_visits_by_leaning

n_cons_visits <- total_visits_by_leaning %>% filter(seed_leaning == 'R') %>% pull(n)
n_lib_visits <- total_visits_by_leaning %>% filter(seed_leaning == 'L') %>% pull(n)

leaning_info %>%
  summarise(isolation = sum((p_r * n_r) / n_cons_visits, na.rm = TRUE)) %>%
  pull(isolation) -> cons_exposure

leaning_info %>%
  summarise(isolation = sum((p_l * n_r)/n_lib_visits, na.rm = TRUE)) %>%
  pull(isolation) -> lib_exposure

isolation_index = cons_exposure - lib_exposure
isolation_index

```






