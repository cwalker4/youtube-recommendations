{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "This notebook does some pre-processing of the unstructured data gathered by the scraper."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import os\n",
    "from datetime import date\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "\n",
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "\n",
    "KEY_LOC = '../../credentials/api_key.txt'\n",
    "with open(KEY_LOC, 'r') as f:\n",
    "    DEVELOPER_KEY = f.read()\n",
    "\n",
    "youtube = build('youtube', 'v3', developerKey=DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scrape_path = '../../data/scrape_results'\n",
    "outdir = '../../data/derived_data/analysis'\n",
    "queries = False\n",
    "\n",
    "if not os.path.isdir(outdir):\n",
    "    os.mkdir(outdir)\n",
    "\n",
    "# import the raw video info json\n",
    "video_info = pd.read_json(os.path.join(scrape_path, 'video_info.json'),\n",
    "                   orient='index')\n",
    "video_info.reset_index(inplace=True)\n",
    "video_info.rename(index=str, columns={\"index\": \"video_id\"}, inplace=True)\n",
    "video_info.to_csv(os.path.join(outdir, 'video_info.csv'),\n",
    "                index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make an adjacency list \n",
    "\n",
    "Combine the BFS tree searches into one big ol' adjacency list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of out-edges\n",
    "adj_dict = {}\n",
    "\n",
    "# populate the dictionary\n",
    "for folder in os.listdir(scrape_path):\n",
    "    if not os.path.isdir(os.path.join(scrape_path, folder)):\n",
    "        continue\n",
    "        \n",
    "    with open(os.path.join(scrape_path, folder, 'search_info.json'), 'r') as f:\n",
    "        search_info = json.load(f)\n",
    "        \n",
    "    for video_id in search_info:\n",
    "        if video_id in adj_dict:\n",
    "            adj_dict[video_id].union(set(search_info[video_id]['recommendations']))\n",
    "        else:\n",
    "            adj_dict[video_id] = set(search_info[video_id]['recommendations'])\n",
    "\n",
    "# save as adjacency list\n",
    "f = open(os.path.join(outdir, 'adjacency_list.txt'), 'w')\n",
    "for video_id in adj_dict:\n",
    "    line = \"{} {}\".format(video_id, \" \".join(adj_dict[video_id]))\n",
    "    f.write(line + \"\\n\")\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make a channel adjacency list\n",
    "\n",
    "Since our classification of channels is based on _channels_, not _videos_ it makes more sense to analyze the _channel_ recommendation graph when it comes to political leaning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up a dataframe of out-edges for each video_id\n",
    "adj_df = pd.DataFrame.from_dict(adj_dict, orient='index')\\\n",
    "         .reset_index()\\\n",
    "         .rename(columns={'index': 'video_id'})\n",
    "#adj_df['out_edges'] = adj_df[[0, 1]].values.tolist().drop(columns=[0,1])\n",
    "\n",
    "# wide -> long\n",
    "adj_df = pd.melt(adj_df, id_vars=['video_id'], value_vars=[0,1], var_name='child_no',\n",
    "                 value_name='child_id')\n",
    "\n",
    "# join in channel information\n",
    "adj_df = adj_df.merge(video_info[['video_id', 'channel']], on='video_id')\\\n",
    "    .rename(columns={'channel': 'parent_channel'})\\\n",
    "    .merge(video_info[['video_id', 'channel']], left_on='child_id', right_on='video_id', suffixes=('', '_right'))\\\n",
    "    .drop(columns=['video_id_right'])\\\n",
    "    .rename(columns={'channel': 'child_channel'})\n",
    "    \n",
    "# aggregate to channel level\n",
    "channel_adj = adj_df.filter(['parent_channel', 'child_channel'])\\\n",
    "                    .groupby(['parent_channel'])\\\n",
    "                    .agg(lambda x: list(x))\\\n",
    "                    .assign(out_deg = len('child_channel'))\\\n",
    "                    .reset_index()\n",
    "                \n",
    "# save as csv\n",
    "channel_adj.to_csv(os.path.join(outdir, 'channel_adj.csv'), index=False)\n",
    "\n",
    "# save as comma-delimited adjacency list\n",
    "f = open(os.path.join(outdir, 'channel_adj.txt'), 'w')\n",
    "for row in channel_adj.iterrows():\n",
    "    row_data = row[1]\n",
    "    line = '{},{}'.format(row_data.parent_channel, \",\".join(row_data.child_channel))\n",
    "    f.write(line + '\\n')\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make search info dataframe\n",
    "\n",
    "The json search_info format was nice for scraping, not nice for analyzing. Pack it into one csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['video_id', 'recommendations', 'depth', 'search', 'query',\n",
    "           'search_splits', 'search_depth', 'root_video']\n",
    "result = pd.DataFrame(columns=columns)\n",
    "\n",
    "for folder in os.listdir(scrape_path):\n",
    "    if not os.path.isdir(os.path.join(scrape_path, folder)):\n",
    "        continue\n",
    "    \n",
    "    with open(os.path.join(scrape_path, folder, 'params.json'), 'r') as f:\n",
    "        params = json.load(f)\n",
    "    \n",
    "    filepath = os.path.join(scrape_path, folder, 'search_info.json')\n",
    "    search_df = pd.read_json(filepath, orient='index').reset_index()\n",
    "    search_df.rename(index=str, columns={'index': 'video_id'}, inplace=True)\n",
    "    \n",
    "    search_df['search'] = folder\n",
    "    search_df['search_splits'] = params['n_splits']\n",
    "    search_df['search_depth'] = params['depth']\n",
    "    search_df['scrape_date'] = params['date']\n",
    "    \n",
    "    if queries:\n",
    "        search_df['query'] = folder.split(\"_\")[0]\n",
    "        search_df['root_video'] = folder.split(\"_\")[-1]\n",
    "    else:\n",
    "        search_df['root_video'] = folder\n",
    "    \n",
    "    result = result.append(search_df, ignore_index=True)\n",
    "    \n",
    "result.to_csv(os.path.join(outdir, 'search_info.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In-degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open adjacency list\n",
    "f = open(os.path.join(outdir, 'adjacency_list.txt'), 'r')\n",
    "\n",
    "in_degrees = {}\n",
    "out_degrees = {}\n",
    "\n",
    "for line in f.read().splitlines():\n",
    "    out_degrees[line.split(\" \")[0]] = len(line.split(\" \")[1:])\n",
    "    for ix, video_id in enumerate(line.split(\" \")):\n",
    "        if ix == 0 or video_id == \"\":\n",
    "            continue\n",
    "        if video_id in in_degrees:\n",
    "            in_degrees[video_id] += 1\n",
    "        else:\n",
    "            in_degrees[video_id] = 1\n",
    "            \n",
    "in_deg = pd.DataFrame.from_dict(in_degrees, orient=\"index\")\n",
    "in_deg = in_deg.rename(index=str, columns={0: 'in_deg'})\n",
    "\n",
    "out_deg = pd.DataFrame.from_dict(out_degrees, orient=\"index\")\n",
    "out_deg = out_deg.rename(index=str, columns={0: 'out_deg'})\n",
    "\n",
    "full = in_deg.join(out_deg, how='left').reset_index()\\\n",
    "             .rename(index=str, columns={'index': 'video_id'})\n",
    "    \n",
    "full.to_csv(os.path.join(outdir, 'vertex_degrees.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pageranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the graph from adjacency list\n",
    "G = nx.read_adjlist(create_using=nx.DiGraph(), \n",
    "                    path=os.path.join(outdir, \"adjacency_list.txt\"))\n",
    "\n",
    "# load pageranks into a dataframe\n",
    "pr = nx.pagerank(G)\n",
    "pr_df = pd.DataFrame.from_dict(pr, orient=\"index\").reset_index()\\\n",
    "                 .rename(index=str, columns={'index': 'video_id', 0: 'pagerank'})\n",
    "    \n",
    "pr_df.to_csv(os.path.join(outdir, 'pageranks.csv'), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get channel information from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channel_ids = video_info.channel_id.unique()\n",
    "\n",
    "batch_size = 50  # 50 seems to be the API limit per request\n",
    "channel_info = {}\n",
    "for ix in range(0, len(channel_ids), batch_size):\n",
    "    batch = channel_ids[ix: ix+batch_size]\n",
    "    id_str = \",\".join(batch)\n",
    "\n",
    "    for _ in range(10):\n",
    "        try:\n",
    "            response = youtube.channels().list(\n",
    "                id=id_str,\n",
    "                part='snippet,statistics,topicDetails'\n",
    "            ).execute()\n",
    "            break\n",
    "        except:\n",
    "            e = sys.exc_info()[0]\n",
    "            print(e)\n",
    "            time.sleep(1)\n",
    "    else:\n",
    "        print(\"Issues with backend: could not get info for {}\".format(id_str))\n",
    "            \n",
    "    \n",
    "    for result in response.get('items', []):\n",
    "        channel_id = result['id']\n",
    "        # Get channel statistics\n",
    "        try:\n",
    "            statistics = result.get('statistics')\n",
    "            snippet = result.get('snippet')\n",
    "            if result.get('topicDetails', []):\n",
    "                cat_urls = result.get('topicDetails')['topicCategories']\n",
    "                categories = [url.split('/')[-1] for url in cat_urls]\n",
    "            else:\n",
    "                categories = -1\n",
    "        except:\n",
    "            print(\"Could not get info for channel {}\".format(channel_id))\n",
    "        \n",
    "        # Populate the channel dict\n",
    "        channel_info[channel_id] = {\n",
    "            'name': snippet.get('title', -1),\n",
    "            'country': snippet.get('country', -1),\n",
    "            'date_created': snippet.get('publishedAt', -1),\n",
    "            'n_subscribers': statistics.get('subscriberCount', -1),\n",
    "            'n_videos': statistics.get('videoCount', -1),\n",
    "            'n_views': statistics.get('viewCount', -1),\n",
    "            'categories': categories}\n",
    "\n",
    "# save as csv\n",
    "channel_df = pd.DataFrame.from_dict(channel_info, orient='index')\\\n",
    "               .reset_index()\\\n",
    "               .rename(columns={'index': 'channel_id'})\n",
    "channel_df.to_csv(os.path.join(outdir, 'channel_info.csv'), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
