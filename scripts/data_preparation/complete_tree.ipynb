{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('../../data/crawl.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sql = '''\n",
    "SELECT r.* FROM recommendations r\n",
    "LEFT JOIN searches s\n",
    "  ON r.search_id=s.search_id\n",
    "WHERE s.date = '2019-08-31'\n",
    "    AND video_id NOT NULL\n",
    "'''\n",
    "\n",
    "recs = pd.read_sql_query(sql, con)\n",
    "recs = recs.query(\"search_id == 34\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "def collapse_multiples(l, factor):\n",
    "    '''\n",
    "    Collapses `factor` duplicates of element in list into a single element.\n",
    "    e.g. if l = [1,1,2,3,3,3,3] then collapse_multiples(l, 2) returns [1,2,3,3]\n",
    "    \n",
    "    INPUT:\n",
    "        l: list\n",
    "        factor: (int) collapse factor\n",
    "    \n",
    "    OUTPUT:\n",
    "        collapsed list\n",
    "    \n",
    "    '''\n",
    "    Counter(l)\n",
    "    res = []\n",
    "    for elem, n in Counter(l).items():\n",
    "        if n % factor == 0:\n",
    "            n /= factor\n",
    "        res.extend([elem] * int(n))\n",
    "    return res\n",
    "    \n",
    "\n",
    "def complete_tree(df, search_id, n_splits=4, const_depth=5):\n",
    "\n",
    "    n_followed = []\n",
    "    res = df.copy().filter(['video_id', 'recommendation', 'depth'])\n",
    "    # create ids for each vertex\n",
    "    res['vertex_id'] = (res\n",
    "                        .groupby(['depth','video_id'])\n",
    "                        .ngroup())\n",
    "    # video depths and recs as dicts for fast lookup\n",
    "    vid_depths = (df[['video_id', 'depth']]\n",
    "                  .drop_duplicates()\n",
    "                  .set_index('video_id')\n",
    "                  .depth\n",
    "                  .to_dict())\n",
    "    vid_recs = (df[['video_id', 'recommendation']]\n",
    "               .groupby('video_id')\n",
    "               .agg(lambda x: list(x))\n",
    "               .recommendation\n",
    "               .to_dict())\n",
    "    v_id = max(res.vertex_id.values)\n",
    "    prev_recs = []\n",
    "    for depth in df.depth.unique():\n",
    "        # parent_ids == the video_ids for the depth we're currently at\n",
    "        parent_ids = list((res\n",
    "                     .query('depth == @depth')\n",
    "                     .video_id\n",
    "                     .unique()))\n",
    "        # take the list difference (account for duplicates)\n",
    "        truncd_ids = [i for i in prev_recs if not i in parent_ids or parent_ids.remove(i)]\n",
    "        \n",
    "        tic_v = time.time()\n",
    "        for video_id in truncd_ids:\n",
    "            v_id += 1\n",
    "            if video_id is None or video_id not in vid_recs:\n",
    "                continue\n",
    "                \n",
    "            # get the recommendations \n",
    "            recs = vid_recs[video_id]\n",
    "            source_depth = vid_depths[video_id]\n",
    "            if depth >= const_depth:\n",
    "                n_followed.append(len(recs))\n",
    "            \n",
    "            # sample if we  are sampling, but our source recommendations were not sampled\n",
    "            if depth >= const_depth and source_depth < const_depth:\n",
    "                sample_inds = np.random.rand(len(recs)) < (1 / len(recs))\n",
    "                recs = list(np.array(recs)[sample_inds])\n",
    "            to_append_l = [[video_id, depth, v_id, rec] for rec in recs]\n",
    "            to_append = pd.DataFrame(to_append_l,\n",
    "                                    columns=['video_id','depth','vertex_id','recommendation'])\n",
    "            res = res.append(to_append)\n",
    "        toc_v = time.time()\n",
    "        \n",
    "        # update previous recs \n",
    "        prev_recs = list(res\n",
    "            .query('depth == @depth')\n",
    "            .recommendation\n",
    "            .values)\n",
    "        \n",
    "        n_recs_depth = res.query(\"depth == @depth\").shape[0]\n",
    "        print(\"Number of recommendations at depth {}: {}\".format(depth, n_recs_depth))\n",
    "        if depth >= 7:\n",
    "            print(\"Average sampled: {}\".format(sum(n_followed) / len(n_followed)))\n",
    "            return n_followed\n",
    "            break\n",
    "\n",
    "    res = res.assign(search_id=search_id).sort_values(['depth', 'video_id'])\n",
    "\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
