{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = sqlite3.connect('data/crawl.sqlite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recs = pd.read_sql_query(\"SELECT * FROM recommendations\", con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_tree(df, search_id):\n",
    "    prev_recs = set([])\n",
    "    res = df.copy().filter(['video_id', 'recommendation', 'depth'])\n",
    "    const_depth = 5\n",
    "    for depth in df.depth.unique():\n",
    "        # parent_ids == the video_ids for the depth we're currently at. The set difference\n",
    "        # between parent_ids and the recommendations from the previous level gives the set\n",
    "        # of nodes we truncated (i.e. didn't follow recommendations for)\n",
    "        parent_ids = (res\n",
    "                     .query('depth == @depth')\n",
    "                     .video_id\n",
    "                     .values)\n",
    "        truncd_ids = set(prev_recs) - set(parent_ids)\n",
    "        prev_recs = (res\n",
    "                    .query('depth == @depth')\n",
    "                    .recommendation\n",
    "                    .values)\n",
    "        if not truncd_ids:\n",
    "            continue\n",
    "            \n",
    "        # iterate through truncated ids, merging in their recommendations and then appending\n",
    "        # to our result\n",
    "        for video_id in truncd_ids:\n",
    "            if video_id is None:\n",
    "                continue\n",
    "            to_append = (pd.DataFrame.from_dict({video_id: depth}, orient='index')\n",
    "                        .reset_index()\n",
    "                        .rename(columns={'index':'video_id', 0:'depth'}))\n",
    "            # if we're past our point of critical depth, sample\n",
    "            to_merge = (df\n",
    "                       .query('video_id == @video_id'))\n",
    "            source_depth = to_merge.depth.values[0]\n",
    "            \n",
    "            # if we (a) want to be sampling, but (b) our source recommendations were not sampled,\n",
    "            # sample them ourselves\n",
    "            if depth > const_depth and source_depth < const_depth:\n",
    "                # ugly line to get the indices of a random sample of the recommendations\n",
    "                sample_inds = np.where(np.random.rand(to_merge.shape[0]) < 1 / to_merge.shape[0])[0]\n",
    "                to_merge = to_merge.iloc[sample_inds]\n",
    "                \n",
    "            to_append = (to_append\n",
    "                        .merge(to_merge[['video_id', 'recommendation']],\n",
    "                              how='right', on='video_id'))\n",
    "            res = res.append(to_append)\n",
    "    res.assign(search_id=search_id)\n",
    "    return res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
